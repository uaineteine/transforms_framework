@startuml

skinparam classAttributeIconSize 0

class Meta{
    meta_version
    events
    __init__(self, inherit_events)
}

class MetaFrame{
    meta
    log_path
    __init__(self, MultiTable, inherit_events)
    add_event(self, event)
    load(path, format, table_name, frame_type, spark)
    save_events(self)
    copy(self, new_name)
    write(self, path, format, overwrite, spark)
    sample(self, n, frac, seed)
}

class MultiTable{
    df
    frame_type
    table_name
    src_path
    infer_table_name(src_path)
    __init__(self, df, src_path, table_name, frame_type)
    __repr__(self)
    __str__(self)
    columns(self)
    nvars(self)
    nrow(self)
    get_pandas_frame(self)
    get_polars_lazy_frame(self)
    show(self, n, truncate)
    load_native_df(path, format, table_name, frame_type, spark)
    load(path, format, table_name, frame_type, auto_capitalise, spark)
    drop(self, columns)
    distinct(self, subset)
    rename(self, columns, inplace)
    write_native_df(dataframe, path, format, frame_type, overwrite, spark)
    write(self, path, format, overwrite, spark)
    concat(self, new_col_name, columns, sep)
    explode(self, column, sep, outer)
    sample(self, n, frac, seed)
}

class TableCollection{
    tables
    named_tables
    collection_version
    __init__(self, tables)
    select_by_names(self)
    select_by_prefix(self, prefix)
    select_by_suffix(self, suffix)
    select_by_range(self, start_name, end_name)
    get_table_names(self)
    filter_tables(self, filter_func)
    get_table(self, name)
    __getitem__(self, name)
    __setitem__(self, name, table)
    __delitem__(self, name)
    __contains__(self, name)
    __len__(self)
    ntables(self)
    save_events(self, table_names)
    save_all(self, output_dir, spark)
}

class SupplyLoad{
    job
    run
    payload_dir
    supply_load_src
    sample
    sample_frac
    sample_rows
    seed
    __init__(self, json_loc, sample_frac, sample_rows, seed, spark)
    load_supplies(self, spark)
}

class Headername{
    acceptable_format(name)
    __new__(cls, header_name)
}

class NamedList{
    __init__(self, items)
    __repr__(self)
    count(self)
    to_json(self)
    overlap(self, other)
    extend_with(self, other)
}

class ColList{
    __init__(self, items)
    acceptable_format(self)
    acceptable_format_check(self)
}

class Tablename{
    acceptable_format(name)
    __new__(cls, table_name)
}

MultiTable <|-- MetaFrame
TableCollection <|-- SupplyLoad
str <|-- Headername
NamedList <|-- ColList
str <|-- Tablename

note top of MetaFrame
A specialised class that extends multitable to include event logging capabilities for data pipeline operations.

This class combines the functionality of a MultiTable (which handles different DataFrame types like PySpark, 
Pandas, or Polars) with an event logging system that tracks all operations performed on the data.

Attributes:
    meta
        events (List[PipelineEvent]): A list of events that have been logged during the pipeline operations.
        metaframe_version (str): Version identifier for the MetaFrame implementation.
        
Example:
    >>> # Create a MetaFrame from an existing MultiTable
    >>> mf = MultiTable.load("data.parquet", "parquet", "my_table", "pyspark", spark)
    >>> pt = MetaFrame(mf)
    >>> 
    >>> # Add custom events
    >>> event = PipelineEvent("transform", "Applied filter", "Filtered rows where column > 10")
    >>> pt.add_event(event)
    >>> 
    >>> # Save all events to log files
    >>> pt.save_events()
end note

note top of MultiTable
A unified wrapper class for handling DataFrames across different frameworks.

This class provides a consistent interface for working with DataFrames from PySpark, Pandas, 
and Polars. It includes utility methods for accessing DataFrame properties and converting between formats.

Attributes:
    df: The underlying DataFrame (PySpark DataFrame, Pandas DataFrame, or Polars LazyFrame).
    frame_type (str): The type of DataFrame ('pyspark', 'pandas', 'polars').
    table_name (Tablename): The name of the table, validated and formatted.
    src_path (str): The source file path where the data was loaded from.
    
Example:
    >>> # Create from existing DataFrame
    >>> import pandas as pd
    >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'c']})
    >>> mf = MetaFrame(df, "data.csv", "my_table", "pandas")
    >>> 
    >>> # Load from file
    >>> mf = MetaFrame.load("data.parquet", "parquet", "my_table", "pyspark", spark)
    >>> 
    >>> # Access properties
    >>> print(f"Columns: {mf.columns}")
    >>> print(f"Rows: {mf.nrow}")
    >>> print(f"Variables: {mf.nvars}")
end note

note top of TableCollection
A collection manager for multiple MetaFrame objects with dictionary-like access.

This class provides a convenient way to manage multiple MetaFrame instances,
allowing access by name through dictionary-style operations. It maintains both
a list of tables and a dictionary for named access, ensuring consistency between
the two data structures.

Attributes:
    tables (list[MetaFrame]): List of all MetaFrame instances in the collection.
    named_tables (dict): Dictionary mapping table names to MetaFrame instances.
    
Example:
    >>> # Create an empty collection
    >>> pt_collection = TableCollection()
    >>> 
    >>> # Add tables
    >>> pt1 = MetaFrame.load("data1.parquet", "parquet", "table1")
    >>> pt2 = MetaFrame.load("data2.parquet", "parquet", "table2")
    >>> pt_collection["table1"] = pt1
    >>> pt_collection["table2"] = pt2
    >>> 
    >>> # Access tables
    >>> table = pt_collection["table1"]
    >>> table_count = len(pt_collection)
    >>> 
    >>> # Select tables by prefix
    >>> clus_tables = pt_collection.select_by_names("clus_*")
    >>> 
    >>> # Select tables by range
    >>> specific_tables = pt_collection.select_by_names("table1", "table3")
    >>> 
    >>> # Save all events
    >>> pt_collection.save_events()
end note

note top of SupplyLoad
A specialised collection manager for loading and managing supply data from JSON configuration files.

This class extends TableCollection to provide automated loading of multiple data sources
from a JSON configuration file. It's designed for scenarios where you need to load
multiple related datasets (supplies) from a single configuration source.

The JSON configuration should follow this structure:
{
    "supply": [
        {
            "name": "table_name",
            "path": "path/to/data.parquet",
            "format": "parquet"
        },
        ...
    ]
}

Attributes:
    supply_load_src (str): The path to the JSON configuration file.
    
Example:
    >>> # JSON file: supply_config.json
    >>> # {
    >>> #     "supply": [
    >>> #         {"name": "customers", "path": "data/customers.parquet", "format": "parquet"},
    >>> #         {"name": "orders", "path": "data/orders.parquet", "format": "parquet"}
    >>> #     ]
    >>> # }
    >>> 
    >>> supply_loader = SupplyLoad("supply_config.json", spark)
    >>> customers_table = supply_loader["customers"]
    >>> orders_table = supply_loader["orders"]
    >>> 
    >>> # Save events for all loaded tables
    >>> supply_loader.save_events()
end note

note top of Headername
A specialised string class for validating and managing header names.

This class ensures header names:
- Are not empty
- Contain only uppercase letters and digits
- Have no spaces or special characters

Example:
    >>> Headername("CUSTOMERNAME")  # Valid
    >>> Headername("ORDER123")      # Valid
    >>> Headername("CustomerName")  # Invalid (not all caps)
    >>> Headername("ORDER DATE")    # Invalid (contains space)
    >>> Headername("")              # Invalid (empty)
end note

note top of Tablename
A specialised string class for validating and managing table names.

This class extends str to enforce naming conventions for database table identifiers.

Valid table names:
- Must not be empty
- Can be purely numeric (e.g., "123")
- Otherwise, must start with a letter or underscore
- Must contain only alphanumeric characters and underscores

Example:
    >>> Tablename("my_table")        # Valid
    >>> Tablename("_private_table")  # Valid
    >>> Tablename("123")             # Valid
    >>> Tablename("1table")          # Invalid
    >>> Tablename("table-name")      # Invalid
    >>> Tablename("")                # Invalid
end note


@enduml