{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2577f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the directory of the current script\n",
    "# Add the parent directory to sys.path\n",
    "parent_dir = '..'\n",
    "sys.path.append(os.path.abspath(parent_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f159660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as Spark\n",
    "from transformslib.tables.metaframe import MetaFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940e8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = Spark.builder.appName(\"demo\").getOrCreate() #assume databricks made this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe3d5456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = MetaFrame.load(\"test_tables/jobs/prod/job_1/date_table.csv\", format=\"csv\", table_name=\"my_table\", frame_type=\"pyspark\", spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898fab2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"log_info\": {\n",
      "    \"filepath\": \"test_tables/jobs/prod/job_1/date_table.csv\",\n",
      "    \"table_name\": \"my_table\",\n",
      "    \"src_format\": \"csv\"\n",
      "  },\n",
      "  \"uuid\": \"0730b1dd-4a3a-4543-962e-b8bea4c353d5\",\n",
      "  \"timestamp\": \"2025-09-23T02:09:46.541840+00:00\",\n",
      "  \"executed_user\": \"Daniel\",\n",
      "  \"macro_uuid\": null,\n",
      "  \"event_type\": \"load\",\n",
      "  \"event_description\": \"Loaded my_table from test_tables/jobs/prod/job_1/date_table.csv\",\n",
      "  \"meta_version\": \"1.0\",\n",
      "  \"class_type\": \"PipelineEvent\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(df.meta.events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8785f84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "667cb20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "609994a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'name', 'event_date']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef18a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35d84b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+\n",
      "| id|name|event_date|\n",
      "+---+----+----------+\n",
      "+---+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c908d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformslib.tables.collections import SupplyLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7baf103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sampling input method for job_id=1 (no run_id specified)\n",
      "Starting supply loading from: ../test_tables/jobs/prod/job_1/sampling_state.json\n",
      "Loading supplies from new sampling_state.json format\n",
      "Loading table 'positions' from ../test_tables/jobs/prod/job_1/positions.csv (format: csv)\n",
      "Casting columns for table 'positions' to expected schema...\n",
      "Validating schema for table 'positions'...\n",
      "Expected Schema:\n",
      "  age: Int64 -> Int64\n",
      "  name: String -> String\n",
      "  var: String -> String\n",
      "  position: String -> String\n",
      "  skill: String -> String\n",
      "Schema check - Column 'age': expected Int64, got LongType()\n",
      "Schema check - Column 'name': expected String, got StringType()\n",
      "Schema check - Column 'var': expected String, got StringType()\n",
      "Schema check - Column 'position': expected String, got StringType()\n",
      "Schema check - Column 'skill': expected String, got StringType()\n",
      "Schema validation passed for table 'positions'\n",
      "Loading table 'salary' from ../test_tables/jobs/prod/job_1/salary.csv (format: csv)\n",
      "Casting columns for table 'salary' to expected schema...\n",
      "Validating schema for table 'salary'...\n",
      "Expected Schema:\n",
      "  salary: Int64 -> Int64\n",
      "  NAME: String -> String\n",
      "  age: Int64 -> Int64\n",
      "Schema check - Column 'salary': expected Int64, got LongType()\n",
      "Schema check - Column 'NAME': expected String, got StringType()\n",
      "Schema check - Column 'age': expected Int64, got LongType()\n",
      "Schema validation passed for table 'salary'\n",
      "Loading table 'location' from ../test_tables/jobs/prod/job_1/location.csv (format: csv)\n",
      "Casting columns for table 'location' to expected schema...\n",
      "Validating schema for table 'location'...\n",
      "Expected Schema:\n",
      "  name: String -> String\n",
      "  city: String -> String\n",
      "  country: String -> String\n",
      "  region: String -> String\n",
      "Schema check - Column 'name': expected String, got StringType()\n",
      "Schema check - Column 'city': expected String, got StringType()\n",
      "Schema check - Column 'country': expected String, got StringType()\n",
      "Schema check - Column 'region': expected String, got StringType()\n",
      "Schema validation passed for table 'location'\n",
      "Loading table 'array_like' from ../test_tables/jobs/prod/job_1/array_like.csv (format: csv)\n",
      "Casting columns for table 'array_like' to expected schema...\n",
      "Validating schema for table 'array_like'...\n",
      "Expected Schema:\n",
      "  var1: String -> String\n",
      "  var2: String -> String\n",
      "  var3: String -> String\n",
      "Schema check - Column 'var1': expected String, got StringType()\n",
      "Schema check - Column 'var2': expected String, got StringType()\n",
      "Schema check - Column 'var3': expected String, got StringType()\n",
      "Schema validation passed for table 'array_like'\n",
      "Loading table 'decimal_table' from ../test_tables/jobs/prod/job_1/decimal_table.csv (format: csv)\n",
      "Casting columns for table 'decimal_table' to expected schema...\n",
      "Validating schema for table 'decimal_table'...\n",
      "Expected Schema:\n",
      "  value1: String -> String\n",
      "  value2: Float64 -> Float64\n",
      "  value3: Float64 -> Float64\n",
      "Schema check - Column 'value1': expected String, got StringType()\n",
      "Schema validation passed for table 'decimal_table'\n",
      "Loading table 'date_table' from ../test_tables/jobs/prod/job_1/date_table.csv (format: csv)\n",
      "Casting columns for table 'date_table' to expected schema...\n",
      "Validating schema for table 'date_table'...\n",
      "Expected Schema:\n",
      "  id: Int64 -> Int64\n",
      "  name: String -> String\n",
      "  event_date: String -> String\n",
      "Schema check - Column 'id': expected Int64, got LongType()\n",
      "Schema check - Column 'name': expected String, got StringType()\n",
      "Schema check - Column 'event_date': expected String, got StringType()\n",
      "Schema validation passed for table 'date_table'\n",
      "Loading table 'state' from ../test_tables/jobs/prod/job_1/state.csv (format: csv)\n",
      "Casting columns for table 'state' to expected schema...\n",
      "Validating schema for table 'state'...\n",
      "Expected Schema:\n",
      "  city: String -> String\n",
      "  state: String -> String\n",
      "Schema check - Column 'city': expected String, got StringType()\n",
      "Schema check - Column 'state': expected String, got StringType()\n",
      "Schema validation passed for table 'state'\n",
      "Successfully loaded 7 tables\n"
     ]
    }
   ],
   "source": [
    "dfs = SupplyLoad(1, enable_schema_validation=True, use_test_path=True, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e3381e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positions',\n",
       " 'salary',\n",
       " 'location',\n",
       " 'array_like',\n",
       " 'decimal_table',\n",
       " 'date_table',\n",
       " 'state']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.get_table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895ef074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---+--------+------+\n",
      "|age|       name|var|position| skill|\n",
      "+---+-----------+---+--------+------+\n",
      "|  1|   John Doe|  b|   front|  high|\n",
      "|  2| Jane Smith|  d|    back|medium|\n",
      "|  3|Bob Johnson|  e|  middle|   low|\n",
      "|  3|Bob Johnson|  f|   front|   low|\n",
      "|  4|     Twiggy|  b|   front|   low|\n",
      "+---+-----------+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs[\"positions\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b831d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"positions\"].nrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cae86f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Transforms Library: 18 transforms available\n",
      "   Use listatomic() to see all available transforms in a table format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformslib.transforms.atomiclib import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9502ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================================================================================================\n",
      " TRANSFORMS LIBRARY - Available Transform Classes\n",
      "========================================================================================================================\n",
      " Total Transforms: 18\n",
      "========================================================================================================================\n",
      "Transform Name     | Description\n",
      "-------------------+--------------------------------------------------------------------------------------------------\n",
      "ComplexFilter      | Transform class for filtering rows in a DataFrame using a backend-specific condition.\n",
      "ConcatColumns      | Transform class for concatenating multiple columns into a single column.\n",
      "DistinctTable      | Transform class for removing duplicate rows from a DataFrame.\n",
      "DropNAValues       | Transform class for dropping rows with NA/None/Null values in a specified column.\n",
      "DropVariable       | Transform class for removing one or more variables/columns from a DataFrame.\n",
      "ExplodeColumn      | Transform class for exploding a list-like column into multiple rows.\n",
      "ForceCase          | Transform class to force string values in a specified column to upper or lower case.\n",
      "JoinTable          | Transform class for joining two tables in a TableCollection.\n",
      "PartitionByValue   | Transform class for partitioning a DataFrame into multiple tables\n",
      "RenameTable        | Transform class for renaming columns in a DataFrame.\n",
      "ReplaceByCondition | Transform class for replacing values in a column based on a comparison condition.\n",
      "RoundNumber        | Transform class to round numeric values in a specified column to a given number of decimal pla...\n",
      "SimpleFilter       | Transform class for filtering rows in a DataFrame using a simple column comparison.\n",
      "SortTable          | Transform class to sort a table by one or more columns.\n",
      "SubsetTable        | Transform class for subsetting a DataFrame to retain only specified columns.\n",
      "TrimWhitespace     | Transform class for trimming leading and trailing whitespace from string values in a specified...\n",
      "TruncateDate       | Transform class to truncate date values in a specified column to year or month level.\n",
      "UnionTables        | Transform class to perform a union of two tables with matching schemas.\n",
      "========================================================================================================================\n",
      " Use help(ClassName) for detailed information about any transform.\n",
      "========================================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "listatomic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3805bfbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variables not found in DataFrame columns: ['name']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfs \u001b[38;5;241m=\u001b[39m \u001b[43mDropVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\work\\ABS\\DA\\transforms_framework\\transforms_framework\\transformslib\\transforms\\base.py:224\u001b[0m, in \u001b[0;36mTransform.apply\u001b[1;34m(self, supply_frames, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mApply the transformation to the provided supply frames with keyword arguments.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    >>> # Transformation is automatically logged\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Perform error checking before transformation\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_check(supply_frames, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Apply transformation\u001b[39;00m\n\u001b[0;32m    227\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(supply_frames, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\work\\ABS\\DA\\transforms_framework\\transforms_framework\\transformslib\\transforms\\atomiclib.py:52\u001b[0m, in \u001b[0;36mDropVariable.error_check\u001b[1;34m(self, supply_frames, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m missing_vars \u001b[38;5;241m=\u001b[39m [var \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supply_frames[table_name]\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariables not found in DataFrame columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Variables not found in DataFrame columns: ['name']"
     ]
    }
   ],
   "source": [
    "dfs = DropVariable(\"name\").apply(dfs, df=\"positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01cea901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------+------+\n",
      "|age|var|position| skill|\n",
      "+---+---+--------+------+\n",
      "|  1|  b|   front|  high|\n",
      "|  2|  d|    back|medium|\n",
      "|  3|  e|  middle|   low|\n",
      "|  3|  f|   front|   low|\n",
      "|  4|  b|   front|   low|\n",
      "+---+---+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs[\"positions\"].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7709f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll = dfs.select_by_suffix(\"_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "773c1053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decimal_table', 'date_table']\n"
     ]
    }
   ],
   "source": [
    "print(coll.get_table_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
