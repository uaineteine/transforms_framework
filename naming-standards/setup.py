from setuptools import setup, find_packages

requirements = []
long_description = '# Meta Transforms Framework\n\n![py version](https://img.shields.io/badge/python-3.10+-blue) ![Version 0.8.2](https://img.shields.io/badge/version-0.8.2-brightgreen) [![Lint Check](https://github.com/uaineteine/transforms_framework/actions/workflows/lint_check.yaml/badge.svg)](https://github.com/uaineteine/transforms_framework/actions/workflows/lint_check.yaml) [![Dup Checks](https://github.com/uaineteine/transforms_framework/actions/workflows/dup_checks.yaml/badge.svg)](https://github.com/uaineteine/transforms_framework/actions/workflows/dup_checks.yaml) [![Package](https://github.com/uaineteine/transforms_framework/actions/workflows/package.yaml/badge.svg)](https://github.com/uaineteine/transforms_framework/actions/workflows/package.yaml)\n\nThis program provides a data transformation framework for working with tables (DataFrames) in PySpark, Pandas, or Polars. It tracks all transformations and important events in a structured, auditable way using JSON logs.\n\n![Example Output](https://raw.githubusercontent.com/uaineteine/transforms_framework/refs/heads/master/diagrams/example_final_form.png)\n\n## Table of Contents\n\n- [Main Components](#main-components)\n- [Summary](#summary)\n- [Schema Validation](docs/schema_validation.md) **NEW**\n- [Naming](docs/naming.md)\n- [Events](docs/events.md)\n- [Tables](docs/tables.md)\n- [Transforms](docs/transforms.md)\n- [Example Workflows](#example-workflows)\n- [Advanced Table Selection Features](#advanced-table-selection-features)\n- [Event Logging](#event-logging)\n- [Notes](#notes)\n- [Building](#building)\n\n# Main Components\n\n```\n|-- transformslib/\n|   |-- events/\n|   |   |-- jsonlog.py\n|   |   `-- eventlog.py\n|   |   `-- pipeevent.py\n|   |-- tables/\n|   |   |-- collections/\n|   |   |   |-- collection.py\n|   |   |   `-- supply_load.py\n|   |   |-- names/\n|   |   |   |-- colname.py\n|   |   |   |-- lists.py\n|   |   |   `-- tablename.py\n|   |   |-- metaframe.py\n|   |   `-- multitable.py\n|   |-- mapping/\n|   |   |-- maps.py\n|   |   |-- webcanvas.py\n|   |   `-- dag.py\n|   |-- transforms/\n|   |   |-- base.py\n|   |   |-- atomiclib.py\n|   |   |-- macrolib.py\n|   |   `-- reader.py\n|   |-- setup.py\n|   `-- meta.py\n`-- templates/\n    |-- template_load_pipe.py\n    `-- template_custom_transform.py\n```\n\n## Summary\n\nThis framework makes your data pipeline transparent and auditable by logging every important action as a structured JSON event. You can trace exactly what happened to each table, when, and why. The framework provides three main approaches for managing tables:\n\n1. **Direct table loading** with `MetaFrame` for single table operations\n2. **Payload-based loading** with `SupplyLoad` for configuration-driven multi-table loading\n3. **Collection management** with `Tablecollection` for flexible multi-table operations\n\nEach approach provides the same event tracking and audit capabilities while offering different levels of control and automation for your specific use case.\n\n## Example Workflows\n\n### Method 1: Direct Table Loading\n\n1. **Load a table:**\n    ```python\n    from pyspark.sql import SparkSession\n    from pipeline_table import PipelineTable\n    \n    spark = SparkSession.builder.master("local").appName("TransformTest").getOrCreate()\n    tbl = PipelineTable.load(spark=spark, path="test.csv", format="csv", table_name="test_table", frame_type="pyspark")\n    ```\n    Automatically logs a "load" event.\n\n2. **Apply a transformation:**\n    ```python\n    from transforms import DropVariable\n    \n    tbl = DropVariable("age")(tbl)\n    ```\n    Logs a "transform" event describing the column dropped.\n\n3. **Save all events:**\n    ```python\n    tbl.save_events()\n    ```\n    Writes all events to a JSON file for auditing.\n\n### Method 2: Payload-Based Loading (SupplyLoad)\n\nFor scenarios where you need to load multiple tables from a configuration file, you can use the `SupplyLoad` class with a JSON payload:\n\n1. **Create a payload configuration file** (`test_tables/payload.json`):\n    ```json\n    {\n      "job_id": 1,\n      "run_id": 2,\n      "supply": [\n        {\n          "name": "test_table",\n          "format": "csv",\n          "path": "test_tables/test.csv"\n        },\n        {\n          "name": "test_table2",\n          "format": "csv",\n          "path": "test_tables/test2.csv"\n        }\n      ]\n    }\n    ```\n\n2. **Load multiple tables using the payload:**\n    ```python\n    from pyspark.sql import SparkSession\n    from transforms import DropVariable\n    from supply_load import SupplyLoad\n\n    # Create Spark session\n    spark = SparkSession.builder.master("local").appName("TransformTest").getOrCreate()\n\n    # Load pipeline tables from payload\n    supply_frames = SupplyLoad("test_tables/payload.json", spark=spark)\n    print("Original columns:", supply_frames["test_table"].columns)\n\n    # Apply transformations to specific tables\n    supply_frames = DropVariable("age")(supply_frames, df="test_table")\n\n    # Show results\n    print("Transformed columns:", supply_frames["test_table"].columns)\n    supply_frames["test_table"].show()\n\n    # Save events for all tables\n    supply_frames.save_events()\n    ```\n\nThis approach is particularly useful when:\n- You need to load multiple related datasets\n- You want to configure your data sources externally\n- You need to manage complex data pipelines with many input sources\n- You want to version control your data source configurations separately from your code\n\n### Method 3: PipelineTables Collection\n\nFor managing multiple tables with more control and flexibility, use the `PipelineTables` collection:\n\n1. **Load multiple tables individually:**\n    ```python\n    from pyspark.sql import SparkSession\n    from transforms import DropVariable\n    from pipeline_table import PipelineTable, PipelineTables\n\n    # Create Spark session\n    spark = SparkSession.builder.master("local").appName("TransformTest").getOrCreate()\n\n    # Load multiple tables\n    test_table = PipelineTable.load(spark=spark, path="test_tables/test.csv", format="csv", table_name="test_table", frame_type="pyspark")\n    test2_table = PipelineTable.load(spark=spark, path="test_tables/test2.csv", format="csv", table_name="test2_table", frame_type="pyspark")\n    clus_table = PipelineTable.load(spark=spark, path="test_tables/clus_data.csv", format="csv", table_name="clus_data", frame_type="pyspark")\n    \n    # Create collection with initial tables\n    tables_list = [test_table, test2_table, clus_table]\n    pt_collection = PipelineTables(tables_list)\n    \n    print(f"Collection created with {pt_collection.ntables} tables")\n    print(f"Available tables: {list(pt_collection.named_tables.keys())}")\n    ```\n\n2. **Demonstrate collection operations:**\n    ```python\n    # Access tables by name\n    first_table = pt_collection["test_table"]\n    print(f"First table columns: {first_table.columns}")\n    \n    # Check if table exists\n    if "test2_table" in pt_collection:\n        print("test2_table exists in collection")\n    \n    # Apply transforms to specific tables\n    pt_collection = DropVariable("age")(pt_collection["test_table"])\n    \n    print(f"Transformed table columns: {pt_collection[\'test_table\'].columns}")\n    pt_collection["test_table"].show()\n    ```\n\n3. **NEW: Advanced table selection:**\n    ```python\n    # Select tables by patterns (returns references to same objects)\n    clus_tables = pt_collection.select_by_names("clus_*")\n    specific_tables = pt_collection.select_by_names("test_table", "test2_table")\n    \n    # Modifications in selected collection affect original\n    clus_tables["clus_data"] = DropVariable("unwanted_column")(clus_tables["clus_data"])\n    # This change is also reflected in pt_collection["clus_data"]\n    \n    # Convenience methods\n    prefix_tables = pt_collection.select_by_prefix("clus_")\n    suffix_tables = pt_collection.select_by_suffix("_data")\n    range_tables = pt_collection.select_by_range("test_table", "test2_table")\n    \n    # Custom filtering\n    large_tables = pt_collection.filter_tables(lambda t: len(t.df) > 1000)\n    ```\n\n4. **Show all tables and save events:**\n    ```python\n    # Show all tables in collection\n    for i, table in enumerate(pt_collection.tables):\n        print(f"Table {i+1}: {table.table_name} - Columns: {table.columns}")\n        table.df.show()\n    \n    # Save events for all tables in the collection\n    pt_collection.save_events()\n    ```\n\nThis approach provides:\n- Fine-grained control over table loading and management\n- Dictionary-style access to tables by name\n- Easy iteration over all tables in the collection\n- Bulk event saving for all tables\n- Flexibility to add/remove tables dynamically\n- **NEW:** Advanced table selection with pattern matching, wildcards, and custom filters\n- **NEW:** Shared object references ensuring modifications propagate across collections\n\n---\n\n## Advanced Table Selection Features\n\nThe `PipelineTables` class now includes powerful table selection capabilities that return references to the same table objects, ensuring modifications propagate across collections.\n\n### Table Selection Methods\n\n#### `select_by_names(*name_patterns)`\nSelect tables by name patterns, supporting wildcards and exact matches:\n\n```python\n# Wildcard patterns\nclus_tables = pt.select_by_names("clus_*")  # All tables starting with "clus_"\nyear_tables = pt.select_by_names("*_2023")  # All tables ending with "_2023"\ntable_tables = pt.select_by_names("table*")  # All tables starting with "table"\n\n# Exact matches\nspecific_tables = pt.select_by_names("table1", "table3", "clus_data")\n\n# Multiple patterns\nmixed_tables = pt.select_by_names("clus_*", "table1", "*_2023")\n```\n\n#### `select_by_prefix(prefix)`\nConvenience method for selecting tables that start with a specific prefix:\n\n```python\nclus_tables = pt.select_by_prefix("clus_")  # Same as "clus_*"\n```\n\n#### `select_by_suffix(suffix)`\nConvenience method for selecting tables that end with a specific suffix:\n\n```python\nyear_tables = pt.select_by_suffix("_2023")  # Same as "*_2023"\n```\n\n#### `select_by_range(start_name, end_name)`\nSelect tables with names that fall within a lexicographic range:\n\n```python\nrange_tables = pt.select_by_range("table1", "table5")  # Tables with names between table1 and table5\n```\n\n#### `filter_tables(filter_func)`\nAdvanced filtering using custom functions:\n\n```python\n# Filter tables with more than 1000 rows\nlarge_tables = pt.filter_tables(lambda t: len(t.df) > 1000)\n\n# Filter tables with specific column\ntables_with_age = pt.filter_tables(lambda t: "age" in t.columns)\n\n# Filter tables by custom criteria\nrecent_tables = pt.filter_tables(lambda t: "2023" in t.table_name)\n```\n\n### Shared Object References\n\n**Important:** All selection methods return references to the same `PipelineTable` objects. This means:\n\n```python\n# Create a collection\npt_collection = PipelineTables([table1, table2, clus_data])\n\n# Select a subset\nclus_tables = pt_collection.select_by_names("clus_*")\n\n# Modify a table in the subset\nclus_tables["clus_data"] = DropVariable("unwanted_column")(clus_tables["clus_data"])\n\n# The change is reflected in the original collection\nprint(pt_collection["clus_data"].columns)  # Shows the modified columns\n```\n\nThis behavior ensures that:\n- Modifications to tables in selected collections affect the original collection\n- Memory efficiency (no copying of table objects)\n- Consistent state across all collections referencing the same tables\n\n### Utility Methods\n\n#### `get_table_names()`\nGet a list of all table names in the collection:\n\n```python\nnames = pt.get_table_names()\nprint(names)  # [\'table1\', \'table2\', \'clus_data\', ...]\n```\n\n---\n\n## Event Logging\n\n- Events are saved in `events_log/job_1/{table_name}_events.json`.\n- Each event is a JSON object, one per line.\n- This provides a complete audit trail of all actions performed on each table.\n\n**Example JSON output for a transform event:**\n```json\n{\n  "event_type": "transform",\n  "uuid": "7184dc0a-3606-4360-908c-1f6032f44d77",\n  "timestamp": "2025-08-25T04:14:57.862503+00:00",\n  "excuted_user": "Daniel",\n  "sub_type": "pipeline_event",\n  "message": "DropVariable",\n  "description": "Removes specified variable(s) from a dataframe",\n  "name": "DropVariable",\n  "transform_type": "TableTransform",\n  "testable_transform": true,\n  "version_pyspark": "4.0.0",\n  "version_polars": "1.31.0",\n  "version_pandas": "2.3.0",\n  "version_python": "3.10.8",\n  "transform_id": "DropVar",\n  "target_tables": [\n    "test_table"\n  ],\n  "target_variables": [\n    "AGE"\n  ],\n  "created_variables": null,\n  "renamed_variables": null,\n  "deleted_variables": [\n    "AGE"\n  ],\n  "hashed_variables": null\n}\n```\n\n**Example JSON output for a load event:**\n```json\n{\n  "event_type": "load",\n  "uuid": "b2e7c8e2-7d4e-4c7e-8b8e-2f6e7c8e2d4e",\n  "timestamp": "2025-08-10T12:34:56.789012",\n  "log_location": "events_log/job_1/test_table_events.json",\n  "message": "Loaded table from test.csv as csv (pyspark)",\n  "description": "Loaded test_table from test.csv"\n}\n```\n\n## Notes\n\n- All transformations should inherit from `Transform` and implement the `transforms` method.\n- The framework is designed for transparency and auditability in data pipelines by logging every important action as a structured JSON event.\n- The `PipelineTable` class is the main entry point for users who want event tracking functionality.\n- `PipelineTables` provides collection management for multiple tables with dictionary-like access.\n- `SupplyLoad` extends `PipelineTables` to provide automated loading from JSON configuration files.\n\n---\n\n## Building\n\nPrerequisites:\n- Python 3.10 or newer\n- pip, setuptools, and wheel\n\nOn Windows PowerShell:\n\n```powershell\n\n# Ensure build tools are available\npip install -U setuptools wheel\n\n# If you have project dependencies, install them (optional for building)\npip install -r requirements.txt\n\n# Build the package\npython build.py\n```\n\nWhat the build script does (python build.py):\n1. Pre-renders setup.py\n   - Reads requirements.txt (if present) and inlines them into install_requires\n   - Reads readme.md as the long_description for the package\n   - Writes a fully-populated setup.py with the package metadata (name, version, etc.)\n2. Cleans previous artifacts\n   - Deletes the build/ and dist/ folders if they exist\n3. Builds distributions\n   - Runs: python setup.py sdist bdist_wheel\n   - Outputs artifacts to the dist/ directory (e.g., .tar.gz and .whl files)\n4. Builds documentation\n   - If a docs/ folder exists, changes into docs/ and executes builddocs.bat\n   - Returns to the project root when finished\n   - For detailed instructions on building the documentation, see [Documentation Build Process](docs/docsbuild.md).\n\nAfter a successful build, you should see the generated distribution files under `dist/`.\n'

setup(
    name="naming-standards",
    version="1.0.0",
    author="",
    author_email="",
    description="A python package of a working transforms framework",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="",
    packages=find_packages(include=["naming-standards", "naming-standards.*"]),
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.10",
    install_requires=requirements
)
