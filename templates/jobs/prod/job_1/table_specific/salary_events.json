{"log_info": {"filepath": "../test_tables/jobs/prod/job_1/salary.csv", "table_name": "", "src_format": "csv"}, "uuid": "ee496e0a-b29c-4804-86fd-ffeb9ab2c934", "timestamp": "2025-09-22T05:30:16.484099+00:00", "executed_user": "Daniel", "macro_uuid": null, "event_type": "load", "event_description": "Loaded  from ../test_tables/jobs/prod/job_1/salary.csv", "meta_version": "1.0", "class_type": "PipelineEvent"}
{"log_info": {"input_tables": ["salary"], "output_tables": ["salary"], "input_variables": [["salary", "age"]], "output_variables": [], "created_variables": null, "renamed_variables": null, "removed_variables": ["NAME"], "input_row_counts": null, "output_row_counts": null, "input_columns": {"salary": ["salary", "NAME", "age"]}, "output_columns": {"salary": ["salary", "age"]}, "num_input_frames": 1, "num_output_frames": 1}, "uuid": "4b7581da-b53e-4b92-b861-f7b19a572d06", "timestamp": "2025-09-22T05:30:21.489427+00:00", "executed_user": "Daniel", "macro_uuid": null, "event_type": "transform", "event_description": "Subsets a dataframe to retain only specified variable(s)", "meta_version": "1.0", "name": "SubsetTable", "transform_type": "TableTransform", "testable_transform": true, "version_pyspark": "4.0.0", "version_polars": "1.32.3", "version_pandas": "2.3.2", "version_python": "3.10.8", "params": {"df": "salary"}, "transform_id": "SubsetTbl", "target_tables": ["salary"], "target_variables": ["salary", "age"], "class_type": "SubsetTable"}
{"log_info": {"input_tables": ["salary"], "output_tables": ["salary"], "input_variables": [], "output_variables": [], "created_variables": null, "renamed_variables": null, "removed_variables": null, "input_row_counts": {"salary": 6}, "output_row_counts": {"salary": 5}, "input_columns": null, "output_columns": null, "num_input_frames": 1, "num_output_frames": 1}, "uuid": "6f1e0513-1d51-4897-9c4d-2cba4a5083e2", "timestamp": "2025-09-22T05:30:22.843484+00:00", "executed_user": "Daniel", "macro_uuid": null, "event_type": "transform", "event_description": "Removes duplicate rows from a DataFrame", "meta_version": "1.0", "name": "DistinctTable", "transform_type": "TableTransform", "testable_transform": false, "version_pyspark": "4.0.0", "version_polars": "1.32.3", "version_pandas": "2.3.2", "version_python": "3.10.8", "params": {"df": "salary"}, "transform_id": "DistinctTbl", "target_tables": ["salary"], "target_variables": [], "class_type": "DistinctTable"}
{"log_info": {"input_tables": ["salary"], "output_tables": ["salary"], "input_variables": [["salary"]], "output_variables": [["income"]], "created_variables": null, "renamed_variables": null, "removed_variables": null, "input_row_counts": null, "output_row_counts": null, "input_columns": {"salary": ["salary", "age"]}, "output_columns": {"salary": ["income", "age"]}, "num_input_frames": 1, "num_output_frames": 1}, "uuid": "68a3aa10-4d8d-4d1e-b010-6c09c09ca4d1", "timestamp": "2025-09-22T05:30:23.732518+00:00", "executed_user": "Daniel", "macro_uuid": null, "event_type": "transform", "event_description": "Renames specified columns in a dataframe", "meta_version": "1.0", "name": "RenameTable", "transform_type": "TableTransform", "testable_transform": true, "version_pyspark": "4.0.0", "version_polars": "1.32.3", "version_pandas": "2.3.2", "version_python": "3.10.8", "params": {"df": "salary"}, "transform_id": "RenmTbl", "target_tables": ["salary"], "target_variables": ["salary"], "rename_map": {"salary": "income"}, "new_names": ["income"], "class_type": "RenameTable"}
{"log_info": {"input_tables": ["salary"], "output_tables": ["salary"], "input_variables": [], "output_variables": [], "created_variables": null, "renamed_variables": null, "removed_variables": null, "input_row_counts": {"salary": 5}, "output_row_counts": {"salary": 3}, "input_columns": null, "output_columns": null, "num_input_frames": 1, "num_output_frames": 1}, "uuid": "1b9de9f2-c3ef-4f5c-9590-79b603df1ee3", "timestamp": "2025-09-22T05:30:23.931518+00:00", "executed_user": "Daniel", "macro_uuid": null, "event_type": "transform", "event_description": "Filters rows in a dataframe using a backend-specific condition", "meta_version": "1.0", "name": "ComplexFilter", "transform_type": "TableTransform", "testable_transform": false, "version_pyspark": "4.0.0", "version_polars": "1.32.3", "version_pandas": "2.3.2", "version_python": "3.10.8", "params": {"df": "salary"}, "transform_id": "RowFilter", "target_tables": ["salary"], "target_variables": [], "condition_map": {"pyspark": {}}, "backend": "pyspark", "condition_string": "\"pyspark\": lambda df: df.filter(col(\"income\") >= 600)", "class_type": "ComplexFilter"}
{"log_info": {"filepath": "..\\test_tables\\jobs\\prod\\job_1\\output\\salary.parquet", "table_name": {}, "out_format": "parquet"}, "uuid": "81248e61-4562-4c63-9b75-235b70b03679", "timestamp": "2025-09-22T05:30:38.559170+00:00", "executed_user": "Daniel", "macro_uuid": null, "event_type": "write", "event_description": "Wrote table to ..\\test_tables\\jobs\\prod\\job_1\\output\\salary.parquet as parquet (pyspark)", "meta_version": "1.0", "filepath": "..\\test_tables\\jobs\\prod\\job_1\\output\\salary.parquet", "table_name": {}, "src_format": "parquet", "class_type": "PipelineEvent"}
